{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "054d4b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca4d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframe(path):\n",
    "    rdd = sc.textFile(path)\\\n",
    "        .map(lambda line: line.split())\\\n",
    "        .map(lambda words: Row(label = words[0], words = words[1:]))\n",
    "    return spark.createDataFrame(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d510b932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data = load_dataframe(\"20ng-train-all-terms.txt\")\n",
    "test_data = load_dataframe(\"20ng-test-all-terms.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f1f004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    inputCol=\"words\", \n",
    "    outputCol = \"bag_of_words\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc1a4253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vectorizer_transformer = vectorizer.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "849e0658",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bag_of_words = vectorizer_transformer.transform(train_data)\n",
    "test_bag_of_words = vectorizer_transformer.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01a5c391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|label                   |\n",
      "+------------------------+\n",
      "|alt.atheism             |\n",
      "|comp.graphics           |\n",
      "|comp.os.ms-windows.misc |\n",
      "|comp.sys.ibm.pc.hardware|\n",
      "|comp.sys.mac.hardware   |\n",
      "|comp.windows.x          |\n",
      "|misc.forsale            |\n",
      "|rec.autos               |\n",
      "|rec.motorcycles         |\n",
      "|rec.sport.baseball      |\n",
      "|rec.sport.hockey        |\n",
      "|sci.crypt               |\n",
      "|sci.electronics         |\n",
      "|sci.med                 |\n",
      "|sci.space               |\n",
      "|soc.religion.christian  |\n",
      "|talk.politics.guns      |\n",
      "|talk.politics.mideast   |\n",
      "|talk.politics.misc      |\n",
      "|talk.religion.misc      |\n",
      "+------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data.select(\"label\")\\\n",
    "    .distinct()\\\n",
    "    .sort(\"label\")\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96a4572f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "label_indexer = StringIndexer(\n",
    "    inputCol=\"label\", \n",
    "    outputCol=\"label_index\"\n",
    ")\n",
    "label_indexer_transformer = label_indexer.fit(train_bag_of_words)\n",
    "\n",
    "\n",
    "train_bag_of_words = label_indexer_transformer.transform(train_bag_of_words)\n",
    "test_bag_of_words = label_indexer_transformer.transform(test_bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9baacfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "classifier = NaiveBayes(\n",
    "    labelCol=\"label_index\",\n",
    "    featuresCol=\"bag_of_words\",\n",
    "    predictionCol=\"label_index_predicted\"\n",
    ")\n",
    "\n",
    "classifier_transformer = classifier.fit(train_bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7839b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = classifier_transformer.transform(train_bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2991500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/03 03:05:24 WARN DAGScheduler: Broadcasting large task binary with size 12.0 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 27:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/03 03:05:28 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 27 (TID 20): Attempting to kill Python Worker\n",
      "+-----------+---------------------+\n",
      "|label_index|label_index_predicted|\n",
      "+-----------+---------------------+\n",
      "|       17.0|                 17.0|\n",
      "|       17.0|                 17.0|\n",
      "|       17.0|                 17.0|\n",
      "|       17.0|                 17.0|\n",
      "|       17.0|                 17.0|\n",
      "|       17.0|                 17.0|\n",
      "|       17.0|                 17.0|\n",
      "|       17.0|                 17.0|\n",
      "|       17.0|                 17.0|\n",
      "|       17.0|                 17.0|\n",
      "+-----------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_predicted.select(\"label_index\", \"label_index_predicted\")\\\n",
    "    .limit(10)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a15c5bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/03 03:07:02 WARN DAGScheduler: Broadcasting large task binary with size 12.0 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 30:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_index\", \n",
    "    predictionCol=\"label_index_predicted\", \n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = evaluator.evaluate(test_predicted)\n",
    "print(\"Accuracy = {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5384e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"bag_of_words\")\n",
    "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
    "classifier = NaiveBayes(\n",
    "    labelCol=\"label_index\", featuresCol=\"bag_of_words\", predictionCol=\"label_index_predicted\",\n",
    ")\n",
    "pipeline = Pipeline(stages=[vectorizer, label_indexer, classifier])\n",
    "pipeline_model = pipeline.fit(train_data)\n",
    "\n",
    "test_predicted = pipeline_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83b26321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/03 03:12:14 WARN DAGScheduler: Broadcasting large task binary with size 12.0 MiB\n",
      "+-----------+---------------------+\n",
      "|label_index|label_index_predicted|\n",
      "+-----------+---------------------+\n",
      "|       17.0|                 17.0|\n",
      "|       17.0|                 17.0|\n",
      "|       17.0|                 17.0|\n",
      "|       17.0|                 17.0|\n",
      "|       17.0|                 17.0|\n",
      "|       17.0|                 17.0|\n",
      "|       17.0|                 19.0|\n",
      "|       17.0|                 17.0|\n",
      "|       17.0|                 17.0|\n",
      "|       17.0|                 17.0|\n",
      "+-----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predicted.select(\"label_index\", \"label_index_predicted\")\\\n",
    "    .limit(10)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b678c296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/03 03:13:31 WARN DAGScheduler: Broadcasting large task binary with size 12.0 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 45:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "accuracy = evaluator.evaluate(test_predicted)\n",
    "print(\"Accuracy = {:.2f}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba81384b1679b55bd2da427ccfe442672b757e0dec22df09690074fa17857141"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
